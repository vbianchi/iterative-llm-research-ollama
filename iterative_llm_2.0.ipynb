{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json: built-in\n",
      "re: built-in\n",
      "urllib.request: built-in\n",
      "datetime: built-in\n",
      "typing: built-in\n",
      "beautifulsoup4 version: 4.12.3\n",
      "ollama version not available (or not set)\n",
      "IPython version: 8.25.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib.request\n",
    "import datetime\n",
    "from typing import Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "from ollama import chat, ChatResponse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# For version checking of installed packages:\n",
    "import bs4\n",
    "import ollama\n",
    "import IPython\n",
    "\n",
    "# Built-in modules\n",
    "print(\"json: built-in\")\n",
    "print(\"re: built-in\")\n",
    "print(\"urllib.request: built-in\")\n",
    "print(\"datetime: built-in\")\n",
    "print(\"typing: built-in\")\n",
    "\n",
    "# Third-party libraries with version attributes\n",
    "print(\"beautifulsoup4 version:\", bs4.__version__)\n",
    "\n",
    "try:\n",
    "    print(\"ollama version:\", ollama.__version__)\n",
    "except AttributeError:\n",
    "    print(\"ollama version not available (or not set)\")\n",
    "\n",
    "print(\"IPython version:\", IPython.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Research Question ####\n",
      "\n",
      "how to use LLMs in bioinformatics\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat, ChatResponse\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Prompt Baselines ---\n",
    "INITIAL_RESPONSE_PROMPT = (\n",
    "    \"You are an expert on the topic: {research_topic}. Provide an extensive, detailed, and comprehensive answer \"\n",
    "    \"to the research question. In your answer, highlight any areas or gaps that might require further exploration.\"\n",
    ")\n",
    "\n",
    "FIND_GAP_PROMPT = (\n",
    "    \"You are a group of 3 experts on the topic: {research_topic}.\"\n",
    "    \"Think step by step on the following assay: \"\n",
    "    \"<ASSAY>\"\n",
    "    \"{assay}\"\n",
    "    \"</ASSAY>\"\n",
    "    \"Provide one new topic to explore to fill a knowledge gap in the assay.\"\n",
    "    \"Based on the gaps identified in your answer, generate a JSON object with the following keys:\\n\"\n",
    "    '   - \"query\": \"The search query string.\"\\n'\n",
    "    '   - \"aspect\": \"The aspect of the topic being addressed by this query.\"\\n'\n",
    "    '   - \"rationale\": \"Why this query will help fill the gap.\"\\n'\n",
    "    \"Provide only the JSON structure.\"\n",
    ")\n",
    "\n",
    "FINALIZE_RESPONSE_PROMPT = (\n",
    "    \"You are a team of expert on the topic: {research_topic}. Your goal is to analyze the text provided in the <TEXT></TEXT> tags \"\n",
    "    \"and create an extensive, detailed, and comprehensive report using the information provided. Aim to 500 words per section.\"\n",
    "    \"Your thesis is formatted in markdown and have:\\n\"\n",
    "    \"1. Title\\n\"\n",
    "    \"2. Introduction\\n\"\n",
    "    \"3. Discussion\\n\"\n",
    "    \"4. Gaps / Further research\\n\"\n",
    "    \"<TEXT>\"\n",
    "    \"{notes}\"\n",
    "    \"</TEXT>\"\n",
    ")\n",
    "\n",
    "# --- Helper: Remove <THINK> Tags ---\n",
    "def remove_think_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove any text enclosed in <THINK>...</THINK> tags.\n",
    "    The regex is case-insensitive.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<\\s*THINK\\s*>.*?<\\s*/\\s*THINK\\s*>\", \"\", text, flags=re.DOTALL | re.IGNORECASE).strip()\n",
    "\n",
    "# --- Configuration & State Management ---\n",
    "class Configuration:\n",
    "    def __init__(self, ollama_base_url: str, local_llm: str, fetch_full_page: bool,\n",
    "                 max_research_loops: int, max_fetch_pages: int, max_token_per_search: int):\n",
    "        self.ollama_base_url = ollama_base_url\n",
    "        self.local_llm = local_llm\n",
    "        self.fetch_full_page = fetch_full_page\n",
    "        self.max_research_loops = max_research_loops\n",
    "        self.max_fetch_pages = max_fetch_pages\n",
    "        self.max_token_per_search = max_token_per_search\n",
    "\n",
    "def query_local_llm(state: dict, config: Configuration, prompt=\"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate an extensive answer for the research topic.\n",
    "    The answer should also indicate potential gaps for further research.\n",
    "    \"\"\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt}\n",
    "    response: ChatResponse = chat(model=config.local_llm, messages=[message])\n",
    "    initial_response = remove_think_tags(response.message.content.strip())\n",
    "    state[\"initial_response\"] = initial_response\n",
    "    state[\"assay\"] = initial_response\n",
    "    return initial_response\n",
    "\n",
    "def initialize_state(research_topic: str) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize the research state with the given topic.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"research_topic\": research_topic,\n",
    "        \"initial_response\": \"\",       # The original extensive answer.\n",
    "        \"assay\":\"\",\n",
    "        \"search_query\": research_topic\n",
    "    }\n",
    "\n",
    "def extract_json_from_llm_output(text: str):\n",
    "    \"\"\"\n",
    "    Attempt to extract a JSON object from the provided text.\n",
    "    This function supports two formats:\n",
    "      1. JSON enclosed in triple backticks with the tag \"json\" (e.g., ```json { ... } ```).\n",
    "      2. JSON enclosed in triple backticks without the tag (e.g., ``` { ... } ```).\n",
    "      3. A plain JSON string.\n",
    "    \n",
    "    Returns:\n",
    "        Parsed JSON object.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError if no valid JSON structure can be found or parsed.\n",
    "    \"\"\"\n",
    "    # Define patterns for JSON enclosed in triple backticks.\n",
    "    patterns = [\n",
    "        r\"```json\\s*(\\{.*?\\})\\s*```\",  # with \"json\" tag (case-insensitive)\n",
    "        r\"```(\\{.*?\\})```\"             # without the tag\n",
    "    ]\n",
    "    \n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        found = re.findall(pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        if found:\n",
    "            matches.extend(found)\n",
    "    \n",
    "    # If no triple-backtick JSON is found, check if the whole text is JSON.\n",
    "    if not matches:\n",
    "        stripped = text.strip()\n",
    "        if stripped.startswith(\"{\") and stripped.endswith(\"}\"):\n",
    "            matches.append(stripped)\n",
    "    \n",
    "    if not matches:\n",
    "        print(\"<ERROR>:\\n%s\\n</ERROR>\" % text)\n",
    "        raise ValueError(\"No JSON structure found in the provided text.\")\n",
    "    \n",
    "    # Try parsing each candidate.\n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_data = json.loads(match)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    print(\"<ERROR>:\\n%s\\n</ERROR>\" % text)\n",
    "    raise ValueError(\"Found JSON-like structure, but could not parse it.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Configuration(\n",
    "        ollama_base_url=\"http://localhost:11434\",  # Your Ollama URL\n",
    "        local_llm=\"qwq\",                      # Default LLM is \"llama3.2\"\n",
    "        fetch_full_page=True,                      # Fetch full page content if needed\n",
    "        max_research_loops=3,                      # Number of research iterations\n",
    "        max_fetch_pages=5,                         # Number of pages to fetch per search\n",
    "        max_token_per_search=4000                  # Token limit per search processing\n",
    "    )\n",
    "\n",
    "    # Step 1: Get the research question from the user\n",
    "    research_topic = input(\"Enter your research question: \")\n",
    "    print(\"#### Research Question ####\\n\")\n",
    "    print(research_topic)\n",
    "    print(\"\\n\")\n",
    "    state = initialize_state(research_topic)\n",
    "\n",
    "    # Step 2: Generate an initial explanation using the local LLM. This answer will be the first instance of the assay\n",
    "    prompt_initial = INITIAL_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"])\n",
    "    initial_explanation = query_local_llm(state, config, prompt_initial)\n",
    "    print(\"#### Initial Explanation ####\\n\")\n",
    "    print(initial_explanation)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#### Thinking Process ####\\n\")\n",
    "    for i in range(config.max_research_loops):\n",
    "        print(\"  >> Find gap... \\n\")\n",
    "        # Step 3: Evaluate the current assay and generate a follow-up question\n",
    "        prompt_gap = FIND_GAP_PROMPT.format(research_topic=state[\"research_topic\"], assay=state[\"assay\"])\n",
    "        followup_question_llm = query_local_llm(state, config, prompt_gap)\n",
    "        followup_question_json = extract_json_from_llm_output(followup_question_llm)\n",
    "        print(f\"  >> Iteration {i+1} - Follow-up Question: {followup_question_json[\"query\"]}\\n\")\n",
    "\n",
    "        print(\"  >> Generate new response based on follow-up question... \\n\")\n",
    "        # Step 4: Reiterate on a new topic\n",
    "        prompt_follow = INITIAL_RESPONSE_PROMPT.format(research_topic=followup_question_json[\"query\"])\n",
    "        follow_explanation = query_local_llm(state, config, prompt_initial)\n",
    "        print(\"  >> Add new data to notes... \\n\")\n",
    "        # Step 5: Add the data to the assay\n",
    "        state[\"assay\"] =  state[\"assay\"] + follow_explanation\n",
    "\n",
    "    # print(state[\"assay\"])\n",
    "    # Step 6: Ask the LLM to finalize the assay by integrating all gathered information and adding references\n",
    "    prompt_finalize = FINALIZE_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"], notes=state[\"assay\"] )\n",
    "    finalize_text_llm = query_local_llm(state, config, prompt_finalize)\n",
    "    \n",
    "    print(\"#### Final Assay ####\\n\")\n",
    "    #print(finalize_text_llm)\n",
    "    display(Markdown(finalize_text_llm))\n",
    "\n",
    "    # # Step 7: Save the final assay locally\n",
    "    # try:\n",
    "    #     with open(\"final_assay.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #         f.write(final_assay)\n",
    "    #     print(\"\\nFinal assay saved to 'final_assay.txt'.\")\n",
    "    # except Exception as e:\n",
    "    #     print(\"Error saving the final assay:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# What is the state of the art of open source LLMs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
