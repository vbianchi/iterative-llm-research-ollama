{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Research Question ####\n",
      "\n",
      "How do you use LLMs in bioinformatics\n",
      "\n",
      "\n",
      "#### Initial Explanation ####\n",
      "\n",
      "**Using Large Language Models (LLMs) in Bioinformatics: An Extensive Overview**\n",
      "\n",
      "The integration of Large Language Models (LLMs) into bioinformatics represents a transformative approach for addressing complex biological questions by leveraging advanced AI capabilities. Below is an organized exploration of LLM applications, challenges, and gaps in this emerging field.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Applications of LLMs in Bioinformatics**\n",
      "#### **A. Literature Mining and Knowledge Extraction**\n",
      "- **Use Cases**:\n",
      "  - **Summarizing Research**: Condensing scientific papers into concise summaries or identifying key findings.\n",
      "  - **Hypothesis Generation**: Suggesting new research avenues by cross-referencing disparate studies (e.g., linking genes to diseases).\n",
      "  - **Citation Analysis**: Identifying influential works or gaps in the literature for systematic reviews.\n",
      "- **Challenges**:\n",
      "  - **Domain-Specific Terminology**: LLMs require fine-tuning on biomedical corpora (e.g., PubMed) to accurately parse technical terms like \"epigenetic modifiers\" or \"CRISPR-Cas9.\"\n",
      "  - **Bias and Inaccuracy**: Overreliance on outdated or biased studies may propagate errors.\n",
      "\n",
      "#### **B. Sequence Analysis**\n",
      "- **Use Cases**:\n",
      "  - **Protein Structure Prediction**: Enhancing tools like AlphaFold by predicting structural motifs or functional sites (e.g., ESM-2 by Meta’s AI team).\n",
      "  - **Variant Interpretation**: Classifying genetic variants as pathogenic, benign, or uncertain based on literature and databases (e.g., ClinVar).\n",
      "  - **Gene Function Prediction**: Inferring biological roles of uncharacterized genes using sequence homology and functional annotations.\n",
      "- **Challenges**:\n",
      "  - **Data Integration**: Combining sequence data with heterogeneous sources like protein-protein interaction networks requires hybrid models.\n",
      "  - **Scalability**: Handling the sheer volume of genomic data (e.g., whole-genome sequences) demands efficient parallelization.\n",
      "\n",
      "#### **C. Drug Discovery and Design**\n",
      "- **Use Cases**:\n",
      "  - **Lead Compound Identification**: Screening libraries of molecules for drug-like properties or binding affinity to targets.\n",
      "  - **De Novo Molecule Generation**: Creating novel compounds with desired pharmacological traits (e.g., using OpenAI’s \"Drug Disco\" framework).\n",
      "  - **Adverse Effect Prediction**: Anticipating side effects by analyzing chemical structures and prior clinical trial data.\n",
      "- **Challenges**:\n",
      "  - **Physicochemical Constraints**: LLMs must be trained on molecular descriptors to respect binding pocket geometries and solubility rules.\n",
      "  - **Validation**: Predictions require experimental validation, which is time-consuming and costly.\n",
      "\n",
      "#### **D. Clinical and Patient Data Analysis**\n",
      "- **Use Cases**:\n",
      "  - **EHR Pattern Recognition**: Extracting insights from unstructured clinical notes (e.g., identifying comorbidities or drug interactions).\n",
      "  - **Predictive Analytics**: Forecasting disease outcomes using genomic, transcriptomic, and demographic data.\n",
      "  - **Personalized Medicine**: Tailoring treatments based on genetic profiles (e.g., cancer therapy selection).\n",
      "- **Challenges**:\n",
      "  - **Privacy Concerns**: Anonymizing sensitive patient data while maintaining utility for training models.\n",
      "  - **Data Fragmentation**: Integrating heterogeneous datasets from hospitals, labs, and public repositories.\n",
      "\n",
      "#### **E. Education and Outreach**\n",
      "- **Use Cases**:\n",
      "  - **Curriculum Development**: Automating the creation of educational materials or problem sets based on current research.\n",
      "  - **Scientific Communication**: Simplifying complex concepts (e.g., CRISPR mechanisms) for students or policymakers.\n",
      "- **Challenges**:\n",
      "  - **Accuracy and Bias**: Ensuring explanations align with peer-reviewed science to avoid misinformation.\n",
      "\n",
      "#### **F. Synthetic Biology and Systems Modeling**\n",
      "- **Use Cases**:\n",
      "  - **Genetic Circuit Design**: Predicting the behavior of synthetic gene networks by analyzing literature on analogous systems.\n",
      "  - **Pathway Reconstruction**: Mapping metabolic or signaling pathways from text-mined data.\n",
      "- **Challenges**:\n",
      "  - **Dynamic Complexity**: Capturing non-linear interactions in biological systems requires advanced modeling beyond pure language tasks.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Technical Considerations and Gaps**\n",
      "#### **A. Domain-Specific Pretraining**\n",
      "- **Need for Specialization**: General LLMs (e.g., GPT-4) often lack sufficient biomedical training. Models like BioBERT or SciBERT are more effective but still limited in scope.\n",
      "- **Gap**: Development of large-scale, domain-specific datasets tailored to bioinformatics tasks.\n",
      "\n",
      "#### **B. Hybrid Architectures**\n",
      "- **Integration with Traditional Tools**: Combining LLMs with numerical models (e.g., CNNs for sequence analysis) to handle structured data like protein structures or microarray results.\n",
      "- **Gap**: Limited interoperability between LLM frameworks and bioinformatics software (e.g., BLAST, PyTorch).\n",
      "\n",
      "#### **C. Interpretability and Trust**\n",
      "- **Black Box Criticism**: Biologists demand transparency in predictions (e.g., why an LLM labels a variant as pathogenic).\n",
      "- **Gap**: Tools for explainable AI (XAI) in life sciences are underdeveloped compared to other fields like finance.\n",
      "\n",
      "#### **D. Computational Resources**\n",
      "- **High Costs**: Training and deploying large models (e.g., 100B+ parameters) requires significant GPU clusters.\n",
      "- **Gap**: Open-access platforms or cloud credits for researchers in low-resource settings.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Ethical, Legal, and Social Implications**\n",
      "- **Bias Amplification**: Underrepresented populations may face disparities in drug discovery or clinical predictions if training data lacks diversity.\n",
      "- **Misuse Risks**: LLM-generated misinformation (e.g., DIY gene editing guides) could harm public health without proper oversight.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Future Directions**\n",
      "1. **Collaborative Datasets**: Building curated biomedical corpora to improve model accuracy and reduce bias.\n",
      "2. **Federated Learning**: Enabling decentralized training on sensitive clinical data while preserving privacy.\n",
      "3. **Regulatory Frameworks**: Establishing guidelines for LLM validation in regulated fields like drug development.\n",
      "4. **Interdisciplinary Training**: Bridging the gap between AI researchers and domain experts through joint curricula.\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**\n",
      "LLMs have immense potential to accelerate discovery in bioinformatics, but their success hinges on addressing technical, ethical, and accessibility challenges. By fostering collaboration across disciplines and investing in specialized tools, this technology can unlock new frontiers in precision medicine, synthetic biology, and beyond.\n",
      "\n",
      "\n",
      "#### Thinking Process ####\n",
      "\n",
      "  >> Find gap... \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON structure found in the provided text.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 161\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# # Step 7: Save the final assay locally\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m#     with open(\"final_assay.txt\", \"w\", encoding=\"utf-8\") as f:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m#     print(\"Error saving the final assay:\", e)\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 161\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[12], line 133\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    131\u001b[0m prompt_gap \u001b[38;5;241m=\u001b[39m FIND_GAP_PROMPT\u001b[38;5;241m.\u001b[39mformat(research_topic\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresearch_topic\u001b[39m\u001b[38;5;124m\"\u001b[39m], assay\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massay\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    132\u001b[0m followup_question_llm \u001b[38;5;241m=\u001b[39m query_local_llm(state, config, prompt_gap)\n\u001b[1;32m--> 133\u001b[0m followup_question_json \u001b[38;5;241m=\u001b[39m extract_json_from_llm_output(followup_question_llm)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  >> Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Follow-up Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfollowup_question_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  >> Generate new response based on follow-up question... \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 89\u001b[0m, in \u001b[0;36mextract_json_from_llm_output\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     86\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, text, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matches:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JSON structure found in the provided text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Iterate through matches and attempt to parse as JSON\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n",
      "\u001b[1;31mValueError\u001b[0m: No JSON structure found in the provided text."
     ]
    }
   ],
   "source": [
    "from ollama import chat, ChatResponse\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Prompt Baselines ---\n",
    "INITIAL_RESPONSE_PROMPT = (\n",
    "    \"You are an expert on the topic: {research_topic}. Provide an extensive, detailed, and comprehensive answer \"\n",
    "    \"to the research question. In your answer, highlight any areas or gaps that might require further exploration.\"\n",
    ")\n",
    "\n",
    "FIND_GAP_PROMPT = (\n",
    "    \"You are a group of 3 experts on the topic: {research_topic}.\"\n",
    "    \"Think step by step on the following assay: \"\n",
    "    \"<ASSAY>\"\n",
    "    \"{assay}\"\n",
    "    \"</ASSAY>\"\n",
    "    \"Provide one new topic to explore to fill a knowledge gap in the assay.\"\n",
    "    \"Based on the gaps identified in your answer, generate a JSON object with the following keys:\\n\"\n",
    "    '   - \"query\": \"The search query string.\"\\n'\n",
    "    '   - \"aspect\": \"The aspect of the topic being addressed by this query.\"\\n'\n",
    "    '   - \"rationale\": \"Why this query will help fill the gap.\"\\n'\n",
    "    \"Provide only the JSON structure.\"\n",
    ")\n",
    "\n",
    "FINALIZE_RESPONSE_PROMPT = (\n",
    "    \"You are a team of expert on the topic: {research_topic}. Your goal is to analyze the text provided in the <TEXT></TEXT> tags \"\n",
    "    \"and create an extensive, detailed, and comprehensive report using the information provided. Aim to 500 words per section.\"\n",
    "    \"Your thesis is formatted in markdown and have:\\n\"\n",
    "    \"1. Title\\n\"\n",
    "    \"2. Introduction\\n\"\n",
    "    \"3. Discussion\\n\"\n",
    "    \"4. Gaps / Further research\\n\"\n",
    "    \"<TEXT>\"\n",
    "    \"{notes}\"\n",
    "    \"</TEXT>\"\n",
    ")\n",
    "\n",
    "# --- Helper: Remove <THINK> Tags ---\n",
    "def remove_think_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove any text enclosed in <THINK>...</THINK> tags.\n",
    "    The regex is case-insensitive.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<\\s*THINK\\s*>.*?<\\s*/\\s*THINK\\s*>\", \"\", text, flags=re.DOTALL | re.IGNORECASE).strip()\n",
    "\n",
    "# --- Configuration & State Management ---\n",
    "class Configuration:\n",
    "    def __init__(self, ollama_base_url: str, local_llm: str, fetch_full_page: bool,\n",
    "                 max_research_loops: int, max_fetch_pages: int, max_token_per_search: int):\n",
    "        self.ollama_base_url = ollama_base_url\n",
    "        self.local_llm = local_llm\n",
    "        self.fetch_full_page = fetch_full_page\n",
    "        self.max_research_loops = max_research_loops\n",
    "        self.max_fetch_pages = max_fetch_pages\n",
    "        self.max_token_per_search = max_token_per_search\n",
    "\n",
    "def query_local_llm(state: dict, config: Configuration, prompt=\"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate an extensive answer for the research topic.\n",
    "    The answer should also indicate potential gaps for further research.\n",
    "    \"\"\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt}\n",
    "    response: ChatResponse = chat(model=config.local_llm, messages=[message])\n",
    "    initial_response = remove_think_tags(response.message.content.strip())\n",
    "    state[\"initial_response\"] = initial_response\n",
    "    state[\"assay\"] = initial_response\n",
    "    return initial_response\n",
    "\n",
    "def initialize_state(research_topic: str) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize the research state with the given topic.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"research_topic\": research_topic,\n",
    "        \"initial_response\": \"\",       # The original extensive answer.\n",
    "        \"assay\":\"\",\n",
    "        \"search_query\": research_topic\n",
    "    }\n",
    "\n",
    "def extract_json_from_llm_output(text):\n",
    "    # Regular expression pattern to match JSON within triple backticks\n",
    "    pattern = r'```json(\\s*{.*?}\\s*)```'\n",
    "\n",
    "    # Find matches using DOTALL to match across multiple lines\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    if not matches:\n",
    "        raise ValueError(\"No JSON structure found in the provided text.\")\n",
    "\n",
    "    # Iterate through matches and attempt to parse as JSON\n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_data = json.loads(match)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError as e:\n",
    "            # If parsing fails, continue to the next match\n",
    "            continue\n",
    "\n",
    "    # If no valid JSON was parsed, raise an error\n",
    "    raise ValueError(\"Found JSON-like structure, but could not parse it.\")\n",
    "\n",
    "def main():\n",
    "    config = Configuration(\n",
    "        ollama_base_url=\"http://localhost:11434\",  # Your Ollama URL\n",
    "        local_llm=\"deepseek-r1:8b\",                      # Default LLM is \"llama3.2\"\n",
    "        fetch_full_page=True,                      # Fetch full page content if needed\n",
    "        max_research_loops=3,                      # Number of research iterations\n",
    "        max_fetch_pages=5,                         # Number of pages to fetch per search\n",
    "        max_token_per_search=4000                  # Token limit per search processing\n",
    "    )\n",
    "\n",
    "    # Step 1: Get the research question from the user\n",
    "    research_topic = input(\"Enter your research question: \")\n",
    "    print(\"#### Research Question ####\\n\")\n",
    "    print(research_topic)\n",
    "    print(\"\\n\")\n",
    "    state = initialize_state(research_topic)\n",
    "\n",
    "    # Step 2: Generate an initial explanation using the local LLM. This answer will be the first instance of the assay\n",
    "    prompt_initial = INITIAL_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"])\n",
    "    initial_explanation = query_local_llm(state, config, prompt_initial)\n",
    "    print(\"#### Initial Explanation ####\\n\")\n",
    "    print(initial_explanation)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#### Thinking Process ####\\n\")\n",
    "    for i in range(config.max_research_loops):\n",
    "        print(\"  >> Find gap... \\n\")\n",
    "        # Step 3: Evaluate the current assay and generate a follow-up question\n",
    "        prompt_gap = FIND_GAP_PROMPT.format(research_topic=state[\"research_topic\"], assay=state[\"assay\"])\n",
    "        followup_question_llm = query_local_llm(state, config, prompt_gap)\n",
    "        followup_question_json = extract_json_from_llm_output(followup_question_llm)\n",
    "        print(f\"  >> Iteration {i+1} - Follow-up Question: {followup_question_json[\"query\"]}\\n\")\n",
    "\n",
    "        print(\"  >> Generate new response based on follow-up question... \\n\")\n",
    "        # Step 4: Reiterate on a new topic\n",
    "        prompt_follow = INITIAL_RESPONSE_PROMPT.format(research_topic=followup_question_json[\"query\"])\n",
    "        follow_explanation = query_local_llm(state, config, prompt_initial)\n",
    "        print(\"  >> Add new data to notes... \\n\")\n",
    "        # Step 5: Add the data to the assay\n",
    "        state[\"assay\"] =  state[\"assay\"] + follow_explanation\n",
    "\n",
    "    # print(state[\"assay\"])\n",
    "    # Step 6: Ask the LLM to finalize the assay by integrating all gathered information and adding references\n",
    "    prompt_finalize = FINALIZE_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"], notes=state[\"assay\"] )\n",
    "    finalize_text_llm = query_local_llm(state, config, prompt_finalize)\n",
    "    \n",
    "    print(\"#### Final Assay ####\\n\")\n",
    "    #print(finalize_text_llm)\n",
    "    display(Markdown(final_markdown))\n",
    "\n",
    "    # # Step 7: Save the final assay locally\n",
    "    # try:\n",
    "    #     with open(\"final_assay.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #         f.write(final_assay)\n",
    "    #     print(\"\\nFinal assay saved to 'final_assay.txt'.\")\n",
    "    # except Exception as e:\n",
    "    #     print(\"Error saving the final assay:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# What is the state of the art of open source LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
