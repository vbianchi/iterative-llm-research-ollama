{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Research Question ####\n",
      "\n",
      "how to use LLM in bioinformatics\n",
      "\n",
      "\n",
      "#### Initial Explanation ####\n",
      "\n",
      "**The Application and Challenges of Large Language Models (LLMs) in Bioinformatics**\n",
      "\n",
      "**Introduction:**\n",
      "Large Language Models (LLMs) represent a cutting-edge technology with potential applications across diverse fields, including bioinformatics. This field leverages computational methods to analyze biological data, making LLMs an intriguing tool for enhancing research efficiency.\n",
      "\n",
      "**Potential Applications of LLMs in Bioinformatics:**\n",
      "\n",
      "1. **Literature Review and Summarization:**\n",
      "   - LLMs can efficiently summarize vast amounts of scientific literature, aiding researchers by extracting key information on specific genes or proteins.\n",
      "   \n",
      "2. **Protein Interaction Prediction:**\n",
      "   - By analyzing text data, LLMs may infer protein interactions, although they require training on biological context for accuracy.\n",
      "\n",
      "3. **Drug Discovery Assistance:**\n",
      "   - LLMs can potentially aid in identifying compounds interacting with target proteins, though their accuracy and relevance to real-world applications need validation.\n",
      "\n",
      "4. **Data Integration:**\n",
      "   - Combining information from databases like GenBank and UniProt, LLMs can offer synthesized insights into gene functions, enhancing understanding for researchers.\n",
      "\n",
      "5. **Genomic Variant Analysis:**\n",
      "   - These models might analyze genomic variations, aiding in personalized medicine by predicting mutation effects on proteins.\n",
      "\n",
      "6. **Clinical Decision Support:**\n",
      "   - Providing evidence-based support to healthcare professionals, though oversight is necessary to ensure medical advice is reliable and ethical.\n",
      "\n",
      "7. **Ontology Development:**\n",
      "   - LLMs can assist in expanding biological ontologies by processing literature to suggest new terms or relationships.\n",
      "\n",
      "8. **Ethical Policy Analysis:**\n",
      "   - Summarizing debates on technologies like CRISPR, LLMs can offer policymakers a balanced view of current discussions.\n",
      "\n",
      "**Challenges and Considerations:**\n",
      "\n",
      "1. **Interpretability:**\n",
      "   - Researchers need clear explanations from LLMs to understand their outputs and maintain trust in their conclusions.\n",
      "\n",
      "2. **Data Quality:**\n",
      "   - The effectiveness of LLMs hinges on the quality and reliability of their training data, requiring robust validation processes.\n",
      "\n",
      "3. **Generalization and Transferability:**\n",
      "   - Models trained on specific datasets may struggle with novel scenarios, necessitating additional testing for cross-domain applicability.\n",
      "\n",
      "4. **Workflow Integration:**\n",
      "   - Adapting LLMs into existing bioinformatics pipelines requires considering how they fit with tools like Python libraries or R packages.\n",
      "\n",
      "**Research Gaps and Future Directions:**\n",
      "\n",
      "- **Accuracy in Interpretation:** Comparing the effectiveness of LLMs with traditional methods in biological text understanding.\n",
      "- **Best Practices:** Developing guidelines for data accuracy, model transparency, and responsible AI use.\n",
      "- **Reproducibility:** Ensuring studies using LLMs are easily reproducible by sharing details on model architecture and training data.\n",
      "- **User Training:** Providing educational resources to help bioinformaticians effectively utilize LLMs without over-reliance.\n",
      "\n",
      "**Conclusion:**\n",
      "LLMs offer significant potential in bioinformatics, bridging AI with advancing biological sciences. However, their application requires addressing challenges like interpretability and data quality, while also considering integration into existing workflows. As this field evolves, further research is needed to maximize the benefits of these models, ensuring they complement traditional approaches effectively.\n",
      "\n",
      "\n",
      "#### Thinking Process ####\n",
      "\n",
      "  >> Find gap... \n",
      "\n",
      "  >> Iteration 1 - Follow-up Question: How can Large Language Models (LLMs) enhance molecular pathway curation through biological literature analysis?\n",
      "\n",
      "  >> Generate new response based on follow-up question... \n",
      "\n",
      "  >> Add new data to notes... \n",
      "\n",
      "  >> Find gap... \n",
      "\n",
      "  >> Iteration 2 - Follow-up Question: How can Large Language Models assist in designing and interpreting biological experiments?\n",
      "\n",
      "  >> Generate new response based on follow-up question... \n",
      "\n",
      "  >> Add new data to notes... \n",
      "\n",
      "  >> Find gap... \n",
      "\n",
      "  >> Iteration 3 - Follow-up Question: How can we enhance interpretability of LLMs in bioinformatics tasks?\n",
      "\n",
      "  >> Generate new response based on follow-up question... \n",
      "\n",
      "  >> Add new data to notes... \n",
      "\n",
      "#### Final Assay ####\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:**\n",
       "\n",
       "Large Language Models (LLMs) are increasingly being utilized in bioinformatics to enhance research efficiency and accuracy through various applications and tools. Here's a structured overview of their current and potential impact:\n",
       "\n",
       "### Current Applications:\n",
       "1. **Text Analysis and Literature Processing:**\n",
       "   - LLMs can summarize research papers, saving researchers time and providing quick insights.\n",
       "   - They assist in generating hypotheses by linking biological concepts, potentially leading to new research ideas.\n",
       "\n",
       "2. **Data Interpretation and Analysis:**\n",
       "   - Complex datasets such as gene expression profiles are analyzed, aiding in identifying patterns and important genes.\n",
       "   - Integration of multi-omics data (genomics, proteomics, metabolomics) may uncover novel connections not easily visible otherwise.\n",
       "\n",
       "3. **Automation of Lab Processes:**\n",
       "   - LLMs support efficient workflow management by aiding in lab experiment planning and outcome prediction, reducing manual task burden.\n",
       "\n",
       "4. **Personalized Medicine:**\n",
       "   - Use of patient-specific data for tailored diagnosis and treatment suggestions, offering more individualized insights compared to traditional methods.\n",
       "\n",
       "5. **Drug Discovery:**\n",
       "   - Prediction of potential drug candidates based on biological interactions, potentially accelerating the process of finding drug leads.\n",
       "\n",
       "6. **Pathway Curation and Experiment Interpretation:**\n",
       "   - Assistance in generating and organizing biological knowledge, aiding hypothesis generation and experimental planning.\n",
       "\n",
       "### Challenges:\n",
       "- **Data Quality and Curation:** Crucial for model reliability; flawed data can lead to misleading outputs.\n",
       "- **Interpretability:** Understanding model decisions is necessary for trust and effective integration into research workflows.\n",
       "- **Bias in Training Data:** Risk of biased outcomes due to incomplete or skewed training datasets.\n",
       "- **Ethical Considerations:** Addressing transparency, accountability, and equitable access to ensure fair use of AI tools.\n",
       "\n",
       "### Future Directions:\n",
       "1. **Integration with Other AI Techniques:**\n",
       "   - Combining LLMs with reinforcement learning or attention mechanisms may enhance their capabilities, creating more sophisticated tools.\n",
       "\n",
       "2. **Improving Interpretability:**\n",
       "   - Developing techniques to clarify model decision processes will foster trust and confidence in their use.\n",
       "\n",
       "3. **Literature Review Assistance:**\n",
       "   - Facilitating systematic reviews and meta-analyses through data extraction and summarization can aid literature-based research.\n",
       "\n",
       "4. **Interdisciplinary Collaboration:**\n",
       "   - Bridging gaps between biologists and computer scientists via unified interfaces will support more effective tool development and utilization.\n",
       "\n",
       "### Conclusion:\n",
       "While LLMs hold significant potential in bioinformatics, overcoming challenges related to data quality, interpretability, bias, and ethics is essential for their effective use. Addressing these issues will drive advancements and applications in the field, ultimately benefiting research and innovation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ollama import chat, ChatResponse\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Prompt Baselines ---\n",
    "INITIAL_RESPONSE_PROMPT = (\n",
    "    \"You are an expert on the topic: {research_topic}. Provide an extensive, detailed, and comprehensive answer \"\n",
    "    \"to the research question. In your answer, highlight any areas or gaps that might require further exploration.\"\n",
    ")\n",
    "\n",
    "FIND_GAP_PROMPT = (\n",
    "    \"You are a group of 3 experts on the topic: {research_topic}.\"\n",
    "    \"Think step by step on the following assay: \"\n",
    "    \"<ASSAY>\"\n",
    "    \"{assay}\"\n",
    "    \"</ASSAY>\"\n",
    "    \"Provide one new topic to explore to fill a knowledge gap in the assay.\"\n",
    "    \"Based on the gaps identified in your answer, generate a JSON object with the following keys:\\n\"\n",
    "    '   - \"query\": \"The search query string.\"\\n'\n",
    "    '   - \"aspect\": \"The aspect of the topic being addressed by this query.\"\\n'\n",
    "    '   - \"rationale\": \"Why this query will help fill the gap.\"\\n'\n",
    "    \"Provide only the JSON structure.\"\n",
    ")\n",
    "\n",
    "FINALIZE_RESPONSE_PROMPT = (\n",
    "    \"You are a team of expert on the topic: {research_topic}. Your goal is to analyze the text provided in the <TEXT></TEXT> tags \"\n",
    "    \"and create an extensive, detailed, and comprehensive report using the information provided. Aim to 500 words per section.\"\n",
    "    \"Your thesis is formatted in markdown and have:\\n\"\n",
    "    \"1. Title\\n\"\n",
    "    \"2. Introduction\\n\"\n",
    "    \"3. Discussion\\n\"\n",
    "    \"4. Gaps / Further research\\n\"\n",
    "    \"<TEXT>\"\n",
    "    \"{notes}\"\n",
    "    \"</TEXT>\"\n",
    ")\n",
    "\n",
    "# --- Helper: Remove <THINK> Tags ---\n",
    "def remove_think_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove any text enclosed in <THINK>...</THINK> tags.\n",
    "    The regex is case-insensitive.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<\\s*THINK\\s*>.*?<\\s*/\\s*THINK\\s*>\", \"\", text, flags=re.DOTALL | re.IGNORECASE).strip()\n",
    "\n",
    "# --- Configuration & State Management ---\n",
    "class Configuration:\n",
    "    def __init__(self, ollama_base_url: str, local_llm: str, fetch_full_page: bool,\n",
    "                 max_research_loops: int, max_fetch_pages: int, max_token_per_search: int):\n",
    "        self.ollama_base_url = ollama_base_url\n",
    "        self.local_llm = local_llm\n",
    "        self.fetch_full_page = fetch_full_page\n",
    "        self.max_research_loops = max_research_loops\n",
    "        self.max_fetch_pages = max_fetch_pages\n",
    "        self.max_token_per_search = max_token_per_search\n",
    "\n",
    "def query_local_llm(state: dict, config: Configuration, prompt=\"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate an extensive answer for the research topic.\n",
    "    The answer should also indicate potential gaps for further research.\n",
    "    \"\"\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt}\n",
    "    response: ChatResponse = chat(model=config.local_llm, messages=[message])\n",
    "    initial_response = remove_think_tags(response.message.content.strip())\n",
    "    state[\"initial_response\"] = initial_response\n",
    "    state[\"assay\"] = initial_response\n",
    "    return initial_response\n",
    "\n",
    "def initialize_state(research_topic: str) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize the research state with the given topic.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"research_topic\": research_topic,\n",
    "        \"initial_response\": \"\",       # The original extensive answer.\n",
    "        \"assay\":\"\",\n",
    "        \"search_query\": research_topic\n",
    "    }\n",
    "\n",
    "def extract_json_from_llm_output(text):\n",
    "    # Regular expression pattern to match JSON within triple backticks\n",
    "    pattern = r'```json(\\s*{.*?}\\s*)```'\n",
    "\n",
    "    # Find matches using DOTALL to match across multiple lines\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    if not matches:\n",
    "        print (\"<ERROR>:\\n%s\\n</ERROR>\"%text)\n",
    "        raise ValueError(\"No JSON structure found in the provided text.\")\n",
    "\n",
    "    # Iterate through matches and attempt to parse as JSON\n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_data = json.loads(match)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError as e:\n",
    "            # If parsing fails, continue to the next match\n",
    "            continue\n",
    "\n",
    "    # If no valid JSON was parsed, raise an error\n",
    "    print (\"<ERROR>:\\n%s\\n</ERROR>\"%text)\n",
    "    raise ValueError(\"Found JSON-like structure, but could not parse it.\")\n",
    "\n",
    "def main():\n",
    "    config = Configuration(\n",
    "        ollama_base_url=\"http://localhost:11434\",  # Your Ollama URL\n",
    "        local_llm=\"deepseek-r1:8b\",                      # Default LLM is \"llama3.2\"\n",
    "        fetch_full_page=True,                      # Fetch full page content if needed\n",
    "        max_research_loops=3,                      # Number of research iterations\n",
    "        max_fetch_pages=5,                         # Number of pages to fetch per search\n",
    "        max_token_per_search=4000                  # Token limit per search processing\n",
    "    )\n",
    "\n",
    "    # Step 1: Get the research question from the user\n",
    "    research_topic = input(\"Enter your research question: \")\n",
    "    print(\"#### Research Question ####\\n\")\n",
    "    print(research_topic)\n",
    "    print(\"\\n\")\n",
    "    state = initialize_state(research_topic)\n",
    "\n",
    "    # Step 2: Generate an initial explanation using the local LLM. This answer will be the first instance of the assay\n",
    "    prompt_initial = INITIAL_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"])\n",
    "    initial_explanation = query_local_llm(state, config, prompt_initial)\n",
    "    print(\"#### Initial Explanation ####\\n\")\n",
    "    print(initial_explanation)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#### Thinking Process ####\\n\")\n",
    "    for i in range(config.max_research_loops):\n",
    "        print(\"  >> Find gap... \\n\")\n",
    "        # Step 3: Evaluate the current assay and generate a follow-up question\n",
    "        prompt_gap = FIND_GAP_PROMPT.format(research_topic=state[\"research_topic\"], assay=state[\"assay\"])\n",
    "        followup_question_llm = query_local_llm(state, config, prompt_gap)\n",
    "        followup_question_json = extract_json_from_llm_output(followup_question_llm)\n",
    "        print(f\"  >> Iteration {i+1} - Follow-up Question: {followup_question_json[\"query\"]}\\n\")\n",
    "\n",
    "        print(\"  >> Generate new response based on follow-up question... \\n\")\n",
    "        # Step 4: Reiterate on a new topic\n",
    "        prompt_follow = INITIAL_RESPONSE_PROMPT.format(research_topic=followup_question_json[\"query\"])\n",
    "        follow_explanation = query_local_llm(state, config, prompt_initial)\n",
    "        print(\"  >> Add new data to notes... \\n\")\n",
    "        # Step 5: Add the data to the assay\n",
    "        state[\"assay\"] =  state[\"assay\"] + follow_explanation\n",
    "\n",
    "    # print(state[\"assay\"])\n",
    "    # Step 6: Ask the LLM to finalize the assay by integrating all gathered information and adding references\n",
    "    prompt_finalize = FINALIZE_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"], notes=state[\"assay\"] )\n",
    "    finalize_text_llm = query_local_llm(state, config, prompt_finalize)\n",
    "    \n",
    "    print(\"#### Final Assay ####\\n\")\n",
    "    #print(finalize_text_llm)\n",
    "    display(Markdown(finalize_text_llm))\n",
    "\n",
    "    # # Step 7: Save the final assay locally\n",
    "    # try:\n",
    "    #     with open(\"final_assay.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #         f.write(final_assay)\n",
    "    #     print(\"\\nFinal assay saved to 'final_assay.txt'.\")\n",
    "    # except Exception as e:\n",
    "    #     print(\"Error saving the final assay:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# What is the state of the art of open source LLMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
