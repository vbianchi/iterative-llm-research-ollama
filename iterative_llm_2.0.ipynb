{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Research Question ####\n",
      "\n",
      "how to use LLM in bioinformatics\n",
      "\n",
      "\n",
      "  >> Create initial assay... \n",
      "\n",
      "The application of Large Language Models (LLMs) in bioinformatics has been rapidly gaining attention in recent years due to their ability to process vast amounts of biological data and generate high-quality outputs. LLMs have shown great promise in various applications, including sequence analysis, genome assembly, protein function prediction, and more.\n",
      "\n",
      "In this comprehensive answer, we will delve into the world of LLMs in bioinformatics, highlighting their potential applications, advantages, and limitations. We will also discuss areas that require further exploration to fully harness the power of LLMs in bioinformatics.\n",
      "\n",
      "**Potential Applications of LLMs in Bioinformatics**\n",
      "\n",
      "1. **Sequence Analysis**: LLMs can be trained on large datasets of genomic sequences to predict novel genes, identify regulatory elements, and detect structural variations.\n",
      "2. **Genome Assembly**: LLMs can be used to improve genome assembly by predicting the sequence structure of contigs and identifying gaps in the assembled genome.\n",
      "3. **Protein Function Prediction**: LLMs can be trained on large datasets of protein sequences and structures to predict protein function, identify potential drug targets, and detect post-translational modifications.\n",
      "4. **Gene Expression Analysis**: LLMs can be used to analyze gene expression data from RNA-seq or ChIP-seq experiments to identify differentially expressed genes and regulatory networks.\n",
      "5. **Protein-Ligand Interaction Prediction**: LLMs can be trained on large datasets of protein-ligand interactions to predict binding affinities, identify potential drug targets, and detect off-target effects.\n",
      "\n",
      "**Advantages of Using LLMs in Bioinformatics**\n",
      "\n",
      "1. **Scalability**: LLMs can process vast amounts of biological data, making them ideal for analyzing large-scale genomic or proteomic datasets.\n",
      "2. **Speed**: LLMs can generate high-quality outputs rapidly, reducing the time required for downstream analysis.\n",
      "3. **Accuracy**: LLMs have demonstrated state-of-the-art accuracy in various bioinformatics applications, such as sequence analysis and protein function prediction.\n",
      "4. **Flexibility**: LLMs can be fine-tuned on specific tasks or domains, allowing for personalized applications.\n",
      "\n",
      "**Limitations of Using LLMs in Bioinformatics**\n",
      "\n",
      "1. **Interpretability**: LLMs can be challenging to interpret, making it difficult to understand the reasoning behind their predictions.\n",
      "2. **Data Quality**: LLMs are only as good as the data they are trained on, and poor-quality data can lead to inaccurate predictions.\n",
      "3. **Overfitting**: LLMs can suffer from overfitting, particularly when dealing with small datasets or complex tasks.\n",
      "4. **Explainability**: LLMs lack a clear understanding of their own decision-making processes, making it difficult to identify biases or errors.\n",
      "\n",
      "**Areas for Further Exploration**\n",
      "\n",
      "1. **Integration with Other Bioinformatics Tools**: Integrating LLMs with other bioinformatics tools, such as sequence analysis software or genome assembly tools, can enhance the overall efficiency and accuracy of downstream analyses.\n",
      "2. **Explainability Techniques**: Developing techniques to explain the reasoning behind LLM predictions can improve interpretability and reduce errors in high-stakes applications.\n",
      "3. **Multi-Task Learning**: Training LLMs on multiple tasks simultaneously can lead to better generalization and improved performance across a range of bioinformatics applications.\n",
      "4. **Transfer Learning**: Using pre-trained LLMs as a starting point for downstream applications can accelerate development times while maintaining accuracy.\n",
      "\n",
      "**Future Directions**\n",
      "\n",
      "1. **Advancements in Explainability Techniques**: Developing more sophisticated explainability techniques, such as saliency maps or feature importance scores, can improve the interpretability of LLM predictions.\n",
      "2. **Integration with Other AI Technologies**: Integrating LLMs with other AI technologies, such as reinforcement learning or generative models, can lead to novel applications and improved performance.\n",
      "3. **Biological Understanding of LLM Performance**: Investigating the biological basis of LLM performance can provide insights into how these models work and inform their design and deployment.\n",
      "\n",
      "In conclusion, LLMs have shown great promise in bioinformatics, offering scalability, speed, and accuracy benefits across various applications. However, limitations such as interpretability, data quality, overfitting, and explainability require further exploration to fully harness the power of LLMs in bioinformatics. By addressing these gaps and advancing explainability techniques, integration with other tools, multi-task learning, and transfer learning, we can unlock the full potential of LLMs in bioinformatics and drive innovation in this rapidly evolving field.\n",
      "\n",
      "**Recommendations for Future Research**\n",
      "\n",
      "1. **Develop Explainability Techniques**: Investigate novel explainability techniques to improve the interpretability of LLM predictions.\n",
      "2. **Integrate with Other Bioinformatics Tools**: Develop tools and pipelines that integrate LLMs with other bioinformatics software to enhance downstream analyses.\n",
      "3. **Multi-Task Learning**: Explore multi-task learning approaches to train LLMs on multiple tasks simultaneously.\n",
      "4. **Transfer Learning**: Investigate the effectiveness of transfer learning in accelerating development times while maintaining accuracy across a range of bioinformatics applications.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, this comprehensive review highlights the potential of LLMs in bioinformatics and identifies areas for further exploration to fully harness their power. By addressing limitations and advancing explainability techniques, integrating with other tools, multi-task learning, and transfer learning, we can unlock the full potential of LLMs in bioinformatics and drive innovation in this rapidly evolving field.\n",
      "\n",
      "**References**\n",
      "\n",
      "1. **\"Large Language Models for Bioinformatics: A Review\"** (2022) [Journal Article]\n",
      "2. **\"Explainable AI for Bioinformatics: A Systematic Review\"** (2023) [Journal Article]\n",
      "3. **\"Bioinformatics Pipeline Optimization using Large Language Models\"** (2022) [Conference Paper]\n",
      "4. **\"Multi-Task Learning with Large Language Models in Bioinformatics\"** (2023) [Conference Paper]\n",
      "\n",
      "Note: The references provided are fictional and for demonstration purposes only.\n",
      "\n",
      "\n",
      "#### Thinking Process ####\n",
      "\n",
      ">> Iteration 1\n",
      "\n",
      "  >> Find gap... \n",
      "     > Follow-up Question: Development of Explainable AI techniques for Large Language Models in Bioinformatics\n",
      "     > Web-search query: explainability large language models bioinformatics\n",
      "  >> Query web-literature... \n",
      "Enhanced Query: explainability large language models bioinformatics site:pubmed.ncbi.nlm.nih.gov OR site:sciencedirect.com OR site:doi.org peer-reviewed journal\n",
      "  >> Combine literature to assay... \n",
      "The provided text appears to be a snippet from a scientific article or research paper, likely related to the field of artificial intelligence (AI) and machine learning. Here's a summary of the content:\n",
      "\n",
      "**Title:** Demystifying the Black Box: A Survey on Explainable Artificial Intelligence for Biomedical Applications\n",
      "\n",
      "**Summary:** The article discusses the challenges and limitations of explainable artificial intelligence (XAI) in biomedical applications, where complex medical models need to be interpretable and transparent. The authors highlight several open problems in XAI for the biomedical field, including:\n",
      "\n",
      "1. Lack of consensus on definitions: There is no clear consensus on what it means for a model to be \"interpretable\" or \"explanable\".\n",
      "2. Domain-specific requirements: Different domains have different interpretability requirements, which makes it challenging to develop models that meet these needs.\n",
      "3. Limited understanding of model behavior: Despite advances in AI and machine learning, there is still limited understanding of how models behave and make predictions.\n",
      "\n",
      "**Key concepts:** Explainable artificial intelligence (XAI), biomedical applications, interpretability, transparency, domain-specific requirements.\n",
      "\n",
      "The article seems to be a survey or review of the current state of XAI in biomedical applications, highlighting the challenges and limitations of developing interpretable and transparent models for complex medical problems.\n",
      ">> Iteration 2\n",
      "\n",
      "  >> Find gap... \n",
      "     > Follow-up Question: Llama for multi-class text classification in clinical decision support\n",
      "     > Web-search query: Llama for multi-class text classification in clinical decision support\n",
      "  >> Query web-literature... \n",
      "Enhanced Query: Llama for multi-class text classification in clinical decision support site:pubmed.ncbi.nlm.nih.gov OR site:sciencedirect.com OR site:doi.org peer-reviewed journal\n",
      "  >> Combine literature to assay... \n",
      "This is an article about a new approach to document classification using a deep learning model called \"DocBERT\". The article discusses the limitations of existing approaches, such as Random Forests and XGBoost, and how DocBERT can improve performance on unbalanced classes. It also provides references to related research and funding information.\n",
      "\n",
      "Here are some key points from the article:\n",
      "\n",
      "* DocBERT is a deep learning model that uses BERT (Bidirectional Encoder Representations from Transformers) as its foundation.\n",
      "* The authors argue that existing approaches, such as Random Forests and XGBoost, are limited in their ability to handle unbalanced classes in document classification tasks.\n",
      "* DocBERT uses a novel approach called \"self-attention\" to improve performance on unbalanced classes. This approach allows the model to focus on the most relevant features for each class.\n",
      "* The authors evaluate DocBERT on several benchmarks and show that it outperforms existing approaches on both balanced and unbalanced classes.\n",
      "* The study was funded by the Jiangsu Provincial University Philosophy and Social Science Research Fund.\n",
      "\n",
      "The article is well-written and provides a clear overview of the research. However, it may be more helpful to readers if the abstract was summarized in simpler language:\n",
      "\n",
      "\"The authors introduce DocBERT, a deep learning model that uses BERT as its foundation. They show that DocBERT outperforms existing approaches on both balanced and unbalanced classes in document classification tasks. The study demonstrates the potential of self-attention mechanisms for improving performance on unbalanced classes.\"\n",
      ">> Iteration 3\n",
      "\n",
      "  >> Find gap... \n",
      "     > Follow-up Question: Deep learning models for handling class imbalance in text classification\n",
      "     > Web-search query: Deep learning models for unbalanced text classification\n",
      "  >> Query web-literature... \n",
      "Enhanced Query: Deep learning models for unbalanced text classification site:pubmed.ncbi.nlm.nih.gov OR site:sciencedirect.com OR site:doi.org peer-reviewed journal\n",
      "  >> Combine literature to assay... \n",
      "Based on the query \"Deep learning models for handling class imbalance in text classification,\" we have gathered information from peer-reviewed literature on web search.\n",
      "\n",
      "To provide a more comprehensive understanding of the application of deep learning models like DocBERT in bioinformatics for handling class imbalance in text classification, we need to combine the information from the written assay with the results of the web search. Here is the combined result:\n",
      "\n",
      "Deep learning models have emerged as a powerful tool for handling class imbalance in text classification tasks. In this context, ensemble learning approaches have been widely adopted to improve the performance of deep learning models (1). Ensemble methods involve combining multiple models or features to leverage their strengths and mitigate the weaknesses (2).\n",
      "\n",
      "One effective way to handle class imbalance is through oversampling techniques, which involves generating additional instances for the minority class to balance the dataset (3). Another approach is cost-sensitive methods, which assign different costs to misclassified instances depending on the class labels (4).\n",
      "\n",
      "In addition, text vectorization techniques have been shown to improve the performance of deep learning models in text classification tasks. These techniques involve converting text data into numerical representations that can be processed by machine learning algorithms (5).\n",
      "\n",
      "Moreover, deep neural networks (DNN) have emerged as a dominant player in the field of machine learning, including natural language processing and text classifications (6). Recent years have seen significant improvements in DNN-based models for various domains, including text classification.\n",
      "\n",
      "A comprehensive survey of text classification techniques has demonstrated the value of deep learning models in detecting complex patterns and creating accurate and efficient models (7).\n",
      "\n",
      "References:\n",
      "(1) \"Ensemble deep learning: A review - ScienceDirect\"\n",
      "(2) \"A review of methods for imbalanced multi-label classification\"\n",
      "(3) \"Dealing with Data Imbalance in Text Classification\" (ScienceDirect)\n",
      "(4) \"Cost-sensitive learning for balanced data\"\n",
      "(5) \"Text Vectorization Techniques for Deep Learning Models\" (unspecified source, but discussed in the web search results)\n",
      "(6) \"Deep neural networks (DNN) (LeCun et al., 2015)\"\n",
      "(7) \"A comprehensive survey of text classification techniques and their applications\"\n",
      "\n",
      "Note that while some information from the written assay was used to provide a more comprehensive understanding, only sources identified through the web search were cited in the combined result.\n",
      "#### Final Assay ####\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Title**\n",
       "Handling Class Imbalance in Text Classification using Deep Learning Models: A Comprehensive Review\n",
       "\n",
       "**Introduction**\n",
       "Text classification is a fundamental task in natural language processing (NLP) that has numerous applications in bioinformatics, such as gene expression analysis, protein function prediction, and disease diagnosis. However, one of the major challenges in text classification tasks is class imbalance, where the minority class dominates the majority class. This can lead to biased models that perform poorly on the minority class. In this review, we will discuss the application of deep learning models like DocBERT in bioinformatics for handling class imbalance in text classification.\n",
       "\n",
       "Deep learning models have emerged as a powerful tool for handling class imbalance in text classification tasks. The recent years have seen significant improvements in DNN-based models for various domains, including text classification. In this review, we will discuss the application of deep learning models like DocBERT in bioinformatics and provide an overview of the methods used to handle class imbalance.\n",
       "\n",
       "**Discussion**\n",
       "Class imbalance is a common problem in text classification tasks, where the minority class dominates the majority class. One effective way to handle class imbalance is through oversampling techniques, which involves generating additional instances for the minority class to balance the dataset (1). Another approach is cost-sensitive methods, which assign different costs to misclassified instances depending on the class labels (2).\n",
       "\n",
       "In addition, text vectorization techniques have been shown to improve the performance of deep learning models in text classification tasks. These techniques involve converting text data into numerical representations that can be processed by machine learning algorithms (3). Deep neural networks (DNN) have emerged as a dominant player in the field of machine learning, including natural language processing and text classifications (4).\n",
       "\n",
       "Recent years have seen significant improvements in DNN-based models for various domains, including text classification. A comprehensive survey of text classification techniques has demonstrated the value of deep learning models in detecting complex patterns and creating accurate and efficient models (5).\n",
       "\n",
       "Moreover, ensemble methods have been widely adopted to improve the performance of deep learning models (6). Ensemble methods involve combining multiple models or features to leverage their strengths and mitigate the weaknesses.\n",
       "\n",
       "**Gaps / Further Research**\n",
       "While deep learning models like DocBERT have shown promising results in handling class imbalance in text classification tasks, there are still some gaps that need to be addressed. Firstly, the development of more efficient and scalable algorithms for handling class imbalance is necessary. Secondly, the application of deep learning models to other bioinformatics domains such as protein function prediction and disease diagnosis needs to be explored.\n",
       "\n",
       "Furthermore, there is a need to investigate the effectiveness of different text vectorization techniques in improving the performance of deep learning models in text classification tasks. Additionally, the development of more robust and interpretable ensemble methods for handling class imbalance is necessary.\n",
       "\n",
       "In conclusion, this review has provided an overview of the application of deep learning models like DocBERT in bioinformatics for handling class imbalance in text classification. We have discussed the various methods used to handle class imbalance and highlighted some of the gaps that need to be addressed in future research.\n",
       "\n",
       "References:\n",
       "(1) \"Ensemble deep learning: A review - ScienceDirect\"\n",
       "(2) \"A review of methods for imbalanced multi-label classification\"\n",
       "(3) \"Text Vectorization Techniques for Deep Learning Models\" (unspecified source, but discussed in the web search results)\n",
       "(4) \"Deep neural networks (DNN) (LeCun et al., 2015)\"\n",
       "(5) \"A comprehensive survey of text classification techniques and their applications\"\n",
       "(6) \"A review of methods for imbalanced multi-label classification\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ollama import chat, ChatResponse\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# --- Prompt Baselines ---\n",
    "RESPONSE_PROMPT = (\n",
    "    \"You are an expert on the topic: {research_topic}. Provide an extensive, detailed, and comprehensive answer \"\n",
    "    \"to the research question. In your answer, highlight any areas or gaps that might require further exploration.\"\n",
    ")\n",
    "\n",
    "FIND_GAP_PROMPT = (\n",
    "    \"You are a group of 3 experts on the topic: {research_topic}.\"\n",
    "    \"Think step by step on the following assay: \"\n",
    "    \"<ASSAY>\"\n",
    "    \"{assay}\"\n",
    "    \"</ASSAY>\"\n",
    "    \"Provide one new topic to explore to fill a knowledge gap in the assay.\"\n",
    "    \"Based on the gaps identified in your answer, generate a JSON object with the following keys:\\n\"\n",
    "    '   - \"query\": \"The search query string.\"\\n'\n",
    "    '   - \"web-query\": \"The web search query string.\"\\n'\n",
    "    '   - \"aspect\": \"The aspect of the topic being addressed by this query.\"\\n'\n",
    "    '   - \"rationale\": \"Why this query will help fill the gap.\"\\n'\n",
    "    \"Provide only the JSON structure.\"\n",
    ")\n",
    "\n",
    "COMBINE_PROMPT = (\n",
    "    \"You are a group of 3 experts on the topic: {research_topic}.\"\n",
    "    \"You have to combine together the information from the written assay in <ASSAY></ASSAY> tags, \"\n",
    "    \"with the information gathered from the results of a web search on peer-reviewed literature within the <WEB></WEB> tags.\"\n",
    "    \"When combining them together you are allowed to use only the sources identified through the web search.\"\n",
    "    \"Cite them in the text where appropriate and report them at the bottom.\"\n",
    "    \"<ASSAY>\"\n",
    "    \"{assay}\"\n",
    "    \"</ASSAY>\"\n",
    "    \"<WEB>\"\n",
    "    \"{web_search}\"\n",
    "    \"</WEB>\"\n",
    ")\n",
    "\n",
    "FINALIZE_RESPONSE_PROMPT = (\n",
    "    \"You are a team of expert on the topic: {research_topic}. Your goal is to analyze the text provided in the <TEXT></TEXT> tags \"\n",
    "    \"and create an extensive, detailed, and comprehensive report using the information provided. Aim to 500 words per section.\"\n",
    "    \"Your thesis is formatted in markdown and have:\\n\"\n",
    "    \"1. Title\\n\"\n",
    "    \"2. Introduction\\n\"\n",
    "    \"3. Discussion\\n\"\n",
    "    \"4. Gaps / Further research\\n\"\n",
    "    \"<TEXT>\"\n",
    "    \"{notes}\"\n",
    "    \"</TEXT>\"\n",
    ")\n",
    "\n",
    "def enhance_query_for_scientific_literature(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhance an LLM-generated query to focus on scientific literature by appending \n",
    "    academic-specific filters and keywords.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The original query.\n",
    "        \n",
    "    Returns:\n",
    "        str: The enhanced query.\n",
    "    \"\"\"\n",
    "    # Add filters to restrict results to scientific literature domains and add academic keywords.\n",
    "    filters = \"site:pubmed.ncbi.nlm.nih.gov OR site:sciencedirect.com OR site:doi.org peer-reviewed journal\"\n",
    "    return f\"{query} {filters}\"\n",
    "\n",
    "def duckduckgo_search(query: str, max_results: int = 5, fetch_full_page: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Perform a DuckDuckGo search for the given query.\n",
    "    Optionally fetch full page content.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        max_results (int): Number of results to return.\n",
    "        fetch_full_page (bool): If True, attempt to retrieve the full page content.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with a \"results\" key containing a list of result dicts.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with DDGS() as ddgs:\n",
    "        search_results = list(ddgs.text(query, max_results=max_results))\n",
    "        for r in search_results:\n",
    "            url = r.get(\"href\")\n",
    "            title = r.get(\"title\")\n",
    "            content = r.get(\"body\")\n",
    "            if not all([url, title, content]):\n",
    "                continue\n",
    "            raw_content = content\n",
    "            if fetch_full_page:\n",
    "                try:\n",
    "                    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    response = urllib.request.urlopen(req)\n",
    "                    html = response.read().decode('utf-8', errors='replace')\n",
    "                    raw_content = BeautifulSoup(html, 'html.parser').get_text()\n",
    "                except Exception as e:\n",
    "                    raw_content = content  # fallback to the snippet content\n",
    "            results.append({\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"content\": content,\n",
    "                \"raw_content\": raw_content\n",
    "            })\n",
    "    return {\"results\": results}\n",
    "\n",
    "def search_scientific_literature(query: str, num_results: int = 5, fetch_full_page: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Enhance the given query for scientific literature and perform a DuckDuckGo search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The original query.\n",
    "        num_results (int): Number of results to return.\n",
    "        fetch_full_page (bool): If True, fetch full page content.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Search results as returned by duckduckgo_search.\n",
    "    \"\"\"\n",
    "    enhanced_query = enhance_query_for_scientific_literature(query)\n",
    "    print(\"Enhanced Query:\", enhanced_query)\n",
    "    return duckduckgo_search(enhanced_query, max_results=num_results, fetch_full_page=fetch_full_page)\n",
    "\n",
    "# --- Helper: Remove <THINK> Tags ---\n",
    "def remove_think_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove any text enclosed in <THINK>...</THINK> tags.\n",
    "    The regex is case-insensitive.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"<\\s*THINK\\s*>.*?<\\s*/\\s*THINK\\s*>\", \"\", text, flags=re.DOTALL | re.IGNORECASE).strip()\n",
    "\n",
    "# --- Configuration & State Management ---\n",
    "class Configuration:\n",
    "    def __init__(self, ollama_base_url: str, local_llm: str, fetch_full_page: bool,\n",
    "                 max_research_loops: int, max_fetch_pages: int, max_token_per_search: int):\n",
    "        self.ollama_base_url = ollama_base_url\n",
    "        self.local_llm = local_llm\n",
    "        self.fetch_full_page = fetch_full_page\n",
    "        self.max_research_loops = max_research_loops\n",
    "        self.max_fetch_pages = max_fetch_pages\n",
    "        self.max_token_per_search = max_token_per_search\n",
    "\n",
    "def query_local_llm(state: dict, config: Configuration, prompt=\"\") -> str:\n",
    "    \"\"\"\n",
    "    Generate an extensive answer for the research topic.\n",
    "    The answer should also indicate potential gaps for further research.\n",
    "    \"\"\"\n",
    "    message = {\"role\": \"user\", \"content\": prompt}\n",
    "    response: ChatResponse = chat(model=config.local_llm, messages=[message])\n",
    "    initial_response = remove_think_tags(response.message.content.strip())\n",
    "    state[\"initial_response\"] = initial_response\n",
    "    state[\"assay\"] = initial_response\n",
    "    return initial_response\n",
    "\n",
    "def initialize_state(research_topic: str) -> dict:\n",
    "    \"\"\"\n",
    "    Initialize the research state with the given topic.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"research_topic\": research_topic,\n",
    "        \"initial_response\": \"\",       # The original extensive answer.\n",
    "        \"assay\":\"\",\n",
    "        \"search_query\": research_topic\n",
    "    }\n",
    "\n",
    "def extract_json_from_llm_output(text: str):\n",
    "    \"\"\"\n",
    "    Attempt to extract a JSON object from the provided text.\n",
    "    This function supports two formats:\n",
    "      1. JSON enclosed in triple backticks with the tag \"json\" (e.g., ```json { ... } ```).\n",
    "      2. JSON enclosed in triple backticks without the tag (e.g., ``` { ... } ```).\n",
    "      3. A plain JSON string.\n",
    "    \n",
    "    Returns:\n",
    "        Parsed JSON object.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError if no valid JSON structure can be found or parsed.\n",
    "    \"\"\"\n",
    "    # Define patterns for JSON enclosed in triple backticks.\n",
    "    patterns = [\n",
    "        r\"```json\\s*(\\{.*?\\})\\s*```\",  # with \"json\" tag (case-insensitive)\n",
    "        r\"```(\\{.*?\\})```\"             # without the tag\n",
    "    ]\n",
    "    \n",
    "    matches = []\n",
    "    for pattern in patterns:\n",
    "        found = re.findall(pattern, text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        if found:\n",
    "            matches.extend(found)\n",
    "    \n",
    "    # If no triple-backtick JSON is found, check if the whole text is JSON.\n",
    "    if not matches:\n",
    "        stripped = text.strip()\n",
    "        if stripped.startswith(\"{\") and stripped.endswith(\"}\"):\n",
    "            matches.append(stripped)\n",
    "    \n",
    "    if not matches:\n",
    "        print(\"<ERROR>:\\n%s\\n</ERROR>\" % text)\n",
    "        raise ValueError(\"No JSON structure found in the provided text.\")\n",
    "    \n",
    "    # Try parsing each candidate.\n",
    "    for match in matches:\n",
    "        try:\n",
    "            json_data = json.loads(match)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    print(\"<ERROR>:\\n%s\\n</ERROR>\" % text)\n",
    "    raise ValueError(\"Found JSON-like structure, but could not parse it.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Configuration(\n",
    "        ollama_base_url=\"http://localhost:11434\",  # Your Ollama URL\n",
    "        local_llm=\"llama3.2\",                      # Default LLM is \"llama3.2\"\n",
    "        fetch_full_page=True,                      # Fetch full page content if needed\n",
    "        max_research_loops=3,                      # Number of research iterations\n",
    "        max_fetch_pages=5,                         # Number of pages to fetch per search\n",
    "        max_token_per_search=4000                  # Token limit per search processing\n",
    "    )\n",
    "\n",
    "    # Step 1: Get the research question from the user\n",
    "    research_topic = input(\"Enter your research question: \")\n",
    "    print(\"#### Research Question ####\\n\")\n",
    "    print(research_topic)\n",
    "    print(\"\\n\")\n",
    "    state = initialize_state(research_topic)\n",
    "\n",
    "    # Step 2: Generate an initial explanation using the local LLM. This answer will be the first instance of the assay\n",
    "    print(\"  >> Create initial assay... \\n\")\n",
    "    prompt_initial = RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"])\n",
    "    initial_explanation = query_local_llm(state, config, prompt_initial)\n",
    "    print(initial_explanation)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#### Thinking Process ####\\n\")\n",
    "    for i in range(config.max_research_loops):\n",
    "        print(f\">> Iteration {i+1}\\n\")\n",
    "        print(\"  >> Find gap... \")\n",
    "        # Step 3: Evaluate the current assay and generate a follow-up question\n",
    "        prompt_gap = FIND_GAP_PROMPT.format(research_topic=state[\"research_topic\"], assay=state[\"assay\"])\n",
    "        followup_question_llm = query_local_llm(state, config, prompt_gap)\n",
    "        followup_question_json = extract_json_from_llm_output(followup_question_llm)\n",
    "        print(f\"     > Follow-up Question: {followup_question_json[\"query\"]}\")\n",
    "        print(f\"     > Web-search query: {followup_question_json[\"web-query\"]}\")\n",
    "\n",
    "        # Step 4: Gather literature sources\n",
    "        print(\"  >> Query web-literature... \")\n",
    "        user_query = followup_question_json[\"web-query\"]\n",
    "        web_search = search_scientific_literature(user_query, num_results=5, fetch_full_page=True)\n",
    "        #print(\"\\n--- Search Results ---\")\n",
    "        #print(json.dumps(results, indent=2))\n",
    "\n",
    "        # Step 5: Reiterate on a new topic\n",
    "        print(\"  >> Combine literature to assay... \\n\")\n",
    "        prompt_follow = COMBINE_PROMPT.format(research_topic=followup_question_json[\"query\"], assay=state[\"assay\"], web_search=web_search)\n",
    "        follow_explanation = query_local_llm(state, config, prompt_follow)\n",
    "        print(follow_explanation)\n",
    "        #print(\"  >> Generate new response based on follow-up question... \\n\")\n",
    "        # Step 4: Reiterate on a new topic\n",
    "        #prompt_follow = RESPONSE_PROMPT.format(research_topic=followup_question_json[\"query\"])\n",
    "        #follow_explanation = query_local_llm(state, config, prompt_initial)\n",
    "        #print(\"  >> Add new data to notes... \\n\")\n",
    "        # Step 5: Add the data to the assay\n",
    "        state[\"assay\"] =  state[\"assay\"] + follow_explanation\n",
    "        print(\"  ########## \")\n",
    "\n",
    "    # print(state[\"assay\"])\n",
    "    # Step 6: Ask the LLM to finalize the assay by integrating all gathered information and adding references\n",
    "    prompt_finalize = FINALIZE_RESPONSE_PROMPT.format(research_topic=state[\"research_topic\"], notes=state[\"assay\"] )\n",
    "    finalize_text_llm = query_local_llm(state, config, prompt_finalize)\n",
    "    \n",
    "    print(\"#### Final Assay ####\\n\")\n",
    "    #print(finalize_text_llm)\n",
    "    display(Markdown(finalize_text_llm))\n",
    "\n",
    "    # # Step 7: Save the final assay locally\n",
    "    # try:\n",
    "    #     with open(\"final_assay.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #         f.write(final_assay)\n",
    "    #     print(\"\\nFinal assay saved to 'final_assay.txt'.\")\n",
    "    # except Exception as e:\n",
    "    #     print(\"Error saving the final assay:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# What is the state of the art of open source LLMs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
